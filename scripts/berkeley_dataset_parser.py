#!/usr/bin/env python3
"""
BERKELEY DATASET PARSER - REAL 77GB DATA UTILIZATION
===================================================

Proper parser for the Berkeley UR5 dataset that can actually decode and utilize
the 77GB of robotics demonstration data for training.

Based on analysis of the actual TFRecord structure:
- Images: 448KB encoded images per timestep
- Robot State: 15-dimensional state vector
- Actions: 3D world vector + 3D rotation + gripper action
- Natural Language: Text instructions with embeddings
- Episodes: Variable length sequences (e.g., 71 timesteps)

Author: NIVA Training Team
Date: 2025-09-02  
Status: Production Berkeley Dataset Parser
"""

import os
import tensorflow as tf
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import cv2
from dataclasses import dataclass
import json
import time

@dataclass
class BerkeleyConfig:
    """Configuration for Berkeley dataset processing"""
    dataset_path: str = "/mnt/niva_hot/datasets/berkeley_autolab_ur5/0.1.0"
    batch_size: int = 8
    max_sequence_length: int = 50  # Truncate long episodes
    image_size: Tuple[int, int] = (224, 224)
    use_hand_camera: bool = True
    use_language: bool = True
    num_parallel_calls: int = 8
    prefetch_buffer: int = 4
    max_files_per_split: Optional[int] = None  # None = use all files
    shuffle_buffer_size: int = 1000

class BerkeleyDatasetParser:
    """Advanced parser for Berkeley robotics dataset"""
    
    def __init__(self, config: BerkeleyConfig):
        self.config = config
        
        # Configure TensorFlow for GPU acceleration
        self._configure_tensorflow()
        
        print(f"ü§ñ BERKELEY DATASET PARSER")
        print(f"==========================")
        print(f"üìÇ Dataset: {config.dataset_path}")
        print(f"üîÑ Image size: {config.image_size}")
        print(f"üìè Max sequence: {config.max_sequence_length}")
        print(f"üëÅÔ∏è Hand camera: {config.use_hand_camera}")
        print(f"üó£Ô∏è Language: {config.use_language}")
    
    def _configure_tensorflow(self):
        """Configure TensorFlow for optimal performance"""
        # Enable GPU memory growth
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print(f"üî• TensorFlow GPU configured: {len(gpus)} GPU(s)")
            except RuntimeError as e:
                print(f"‚ö†Ô∏è GPU configuration warning: {e}")
    
    def _get_tfrecord_files(self, split: str = 'train') -> List[str]:
        """Get list of TFRecord files for specified split"""
        pattern = f"berkeley_autolab_ur5-{split}.tfrecord-"
        
        files = []
        for filename in os.listdir(self.config.dataset_path):
            if filename.startswith(pattern):
                files.append(os.path.join(self.config.dataset_path, filename))
        
        files = sorted(files)
        
        if self.config.max_files_per_split:
            files = files[:self.config.max_files_per_split]
        
        print(f"üìÅ Found {len(files)} {split} files")
        return files
    
    def _decode_image(self, encoded_image: tf.Tensor) -> tf.Tensor:
        """Decode and preprocess images"""
        # Decode image (handles PNG/JPEG automatically)
        image = tf.image.decode_image(encoded_image, channels=3)
        
        # Ensure shape is known
        image = tf.reshape(image, [-1, 3])  # Flatten then reshape
        image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0,1]
        
        # We'll handle resizing in PyTorch for better GPU utilization
        return image
    
    def _parse_episode(self, example_proto):
        """Parse a single episode from TFRecord"""
        
        # Define feature specification based on actual structure
        feature_spec = {
            # Images - main camera and hand camera
            'steps/observation/image': tf.io.VarLenFeature(tf.string),
            'steps/observation/hand_image': tf.io.VarLenFeature(tf.string),
            
            # Robot state (15-dimensional)
            'steps/observation/robot_state': tf.io.VarLenFeature(tf.float32),
            
            # Actions 
            'steps/action/world_vector': tf.io.VarLenFeature(tf.float32),
            'steps/action/rotation_delta': tf.io.VarLenFeature(tf.float32),
            'steps/action/gripper_closedness_action': tf.io.VarLenFeature(tf.float32),
            'steps/action/terminate_episode': tf.io.VarLenFeature(tf.float32),
            
            # Episode structure
            'steps/is_first': tf.io.VarLenFeature(tf.int64),
            'steps/is_last': tf.io.VarLenFeature(tf.int64),
            'steps/is_terminal': tf.io.VarLenFeature(tf.int64),
            'steps/reward': tf.io.VarLenFeature(tf.float32),
            
            # Language (optional)
            'steps/observation/natural_language_instruction': tf.io.VarLenFeature(tf.string),
            'steps/observation/natural_language_embedding': tf.io.VarLenFeature(tf.float32),
        }
        
        # Parse the example
        parsed = tf.io.parse_single_example(example_proto, feature_spec)
        
        # Convert sparse tensors to dense
        for key in parsed:
            parsed[key] = tf.sparse.to_dense(parsed[key])
        
        # Extract episode length from is_first (count of timesteps)
        episode_length = tf.shape(parsed['steps/is_first'])[0]
        
        # Limit episode length for memory efficiency
        max_len = tf.minimum(episode_length, self.config.max_sequence_length)
        
        # Extract and process data
        episode_data = {}
        
        # Process images (main camera)
        images = parsed['steps/observation/image'][:max_len]
        # We'll decode images in the PyTorch dataset for better GPU utilization
        episode_data['images'] = images
        
        # Process hand images (if enabled)
        if self.config.use_hand_camera:
            hand_images = parsed['steps/observation/hand_image'][:max_len]
            episode_data['hand_images'] = hand_images
        
        # Process robot state (reshape from flat to [timesteps, state_dim])
        robot_states = parsed['steps/observation/robot_state']
        # The states are flattened, so we need to reshape
        state_dim = 15  # Based on our analysis
        timesteps = tf.shape(robot_states)[0] // state_dim
        robot_states = tf.reshape(robot_states, [timesteps, state_dim])[:max_len]
        episode_data['robot_states'] = robot_states
        
        # Process actions
        world_vectors = parsed['steps/action/world_vector']
        world_vectors = tf.reshape(world_vectors, [-1, 3])[:max_len]  # 3D world vector
        
        rotation_deltas = parsed['steps/action/rotation_delta'] 
        rotation_deltas = tf.reshape(rotation_deltas, [-1, 3])[:max_len]  # 3D rotation
        
        gripper_actions = parsed['steps/action/gripper_closedness_action'][:max_len]
        gripper_actions = tf.expand_dims(gripper_actions, -1)  # Add dimension
        
        # Combine actions [world_vector(3) + rotation_delta(3) + gripper(1)] = 7D
        actions = tf.concat([world_vectors, rotation_deltas, gripper_actions], axis=-1)
        episode_data['actions'] = actions
        
        # Process language instructions (if enabled)
        if self.config.use_language:
            language_instructions = parsed['steps/observation/natural_language_instruction'][:max_len]
            language_embeddings = parsed['steps/observation/natural_language_embedding']
            # Reshape embeddings (they're flattened)
            embedding_dim = 512  # Common dimension
            timesteps_emb = tf.shape(language_embeddings)[0] // embedding_dim
            if timesteps_emb > 0:
                language_embeddings = tf.reshape(language_embeddings, [timesteps_emb, embedding_dim])[:max_len]
            else:
                language_embeddings = tf.zeros([max_len, embedding_dim], dtype=tf.float32)
            
            episode_data['language_instructions'] = language_instructions
            episode_data['language_embeddings'] = language_embeddings
        
        # Episode metadata
        episode_data['episode_length'] = max_len
        episode_data['rewards'] = parsed['steps/reward'][:max_len]
        episode_data['is_terminal'] = tf.cast(parsed['steps/is_terminal'][:max_len], tf.float32)
        
        return episode_data
    
    def create_dataset(self, split: str = 'train') -> tf.data.Dataset:
        """Create TensorFlow dataset for the specified split"""
        
        # Get TFRecord files
        tfrecord_files = self._get_tfrecord_files(split)
        
        if not tfrecord_files:
            raise ValueError(f"No TFRecord files found for split '{split}'")
        
        print(f"üîÑ Creating dataset from {len(tfrecord_files)} files...")
        
        # Create dataset from TFRecord files
        dataset = tf.data.TFRecordDataset(
            tfrecord_files,
            compression_type="",
            buffer_size=8 * 1024 * 1024,  # 8MB buffer
            num_parallel_reads=self.config.num_parallel_calls
        )
        
        # Parse episodes in parallel
        dataset = dataset.map(
            self._parse_episode,
            num_parallel_calls=self.config.num_parallel_calls
        )
        
        # Shuffle if training
        if split == 'train':
            dataset = dataset.shuffle(self.config.shuffle_buffer_size)
        
        # Batch and prefetch
        dataset = dataset.batch(self.config.batch_size, drop_remainder=True)
        dataset = dataset.prefetch(self.config.prefetch_buffer)
        
        print(f"‚úÖ Dataset created successfully")
        return dataset

class BerkeleyPyTorchDataset(Dataset):
    """PyTorch Dataset wrapper for Berkeley data with GPU optimization"""
    
    def __init__(self, config: BerkeleyConfig, split: str = 'train'):
        self.config = config
        self.split = split
        
        # Create TensorFlow dataset
        self.parser = BerkeleyDatasetParser(config)
        self.tf_dataset = self.parser.create_dataset(split)
        
        # Pre-load a reasonable number of episodes for faster training
        print(f"üîÑ Pre-loading episodes for {split} split...")
        self.episodes = []
        
        # Load episodes (limit for memory)
        max_episodes = 1000 if split == 'train' else 200
        
        for batch_idx, batch in enumerate(self.tf_dataset.take(max_episodes // config.batch_size)):
            # Convert TF batch to list of episodes
            batch_episodes = self._tf_batch_to_episodes(batch)
            self.episodes.extend(batch_episodes)
            
            if (batch_idx + 1) % 10 == 0:
                print(f"   Loaded {len(self.episodes)} episodes...")
        
        print(f"‚úÖ Loaded {len(self.episodes)} episodes for {split}")
        
        # Calculate success rate
        successful_episodes = sum(1 for ep in self.episodes if ep['success'])
        self.success_rate = successful_episodes / len(self.episodes) if self.episodes else 0
        print(f"üìä Success rate: {self.success_rate:.1%}")
    
    def _tf_batch_to_episodes(self, tf_batch: Dict[str, tf.Tensor]) -> List[Dict[str, Any]]:
        """Convert TensorFlow batch to list of episodes"""
        batch_size = tf_batch['episode_length'].shape[0]
        episodes = []
        
        for i in range(batch_size):
            episode = {}
            
            # Extract episode length
            ep_len = int(tf_batch['episode_length'][i].numpy())
            
            # Extract and decode images
            images = []
            for j in range(ep_len):
                img_bytes = tf_batch['images'][i][j].numpy()
                if len(img_bytes) > 0:
                    # Decode image using OpenCV (faster than TF for individual images)
                    img_array = np.frombuffer(img_bytes, dtype=np.uint8)
                    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
                    if img is not None:
                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        img = cv2.resize(img, self.config.image_size)
                        img = img.astype(np.float32) / 255.0
                        images.append(img)
            
            if images:
                episode['images'] = np.stack(images)  # [seq_len, H, W, 3]
            else:
                # Create dummy images if decoding failed
                episode['images'] = np.zeros((ep_len, *self.config.image_size, 3), dtype=np.float32)
            
            # Extract other data
            episode['robot_states'] = tf_batch['robot_states'][i][:ep_len].numpy()
            episode['actions'] = tf_batch['actions'][i][:ep_len].numpy() 
            episode['rewards'] = tf_batch['rewards'][i][:ep_len].numpy()
            episode['is_terminal'] = tf_batch['is_terminal'][i][:ep_len].numpy()
            episode['episode_length'] = ep_len
            
            # Determine success (episode has positive reward or terminal state)
            episode['success'] = bool(np.any(episode['rewards'] > 0) or np.any(episode['is_terminal'] > 0))
            
            episodes.append(episode)
        
        return episodes
    
    def __len__(self) -> int:
        return len(self.episodes)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """Get episode as PyTorch tensors"""
        episode = self.episodes[idx]
        
        return {
            'images': torch.from_numpy(episode['images']).float(),  # [seq_len, H, W, 3]
            'robot_states': torch.from_numpy(episode['robot_states']).float(),  # [seq_len, 15]
            'actions': torch.from_numpy(episode['actions']).float(),  # [seq_len, 7]
            'rewards': torch.from_numpy(episode['rewards']).float(),  # [seq_len]
            'episode_length': torch.tensor(episode['episode_length']).long(),
            'success': torch.tensor(episode['success']).float()
        }

def test_berkeley_parser():
    """Test the Berkeley dataset parser"""
    print("üß™ TESTING BERKELEY DATASET PARSER")
    print("=" * 50)
    
    # Create configuration for testing
    config = BerkeleyConfig(
        batch_size=2,
        max_sequence_length=20,
        max_files_per_split=2,  # Only test with 2 files
        image_size=(128, 128)   # Smaller images for testing
    )
    
    try:
        # Test TensorFlow dataset
        parser = BerkeleyDatasetParser(config)
        tf_dataset = parser.create_dataset('train')
        
        print("\nüìä Testing TensorFlow dataset...")
        for batch_idx, batch in enumerate(tf_dataset.take(1)):
            print(f"Batch {batch_idx}:")
            for key, value in batch.items():
                print(f"  {key}: {value.shape} ({value.dtype})")
        
        # Test PyTorch dataset
        print("\nüî• Testing PyTorch dataset...")
        pytorch_dataset = BerkeleyPyTorchDataset(config, 'train')
        
        # Test data loader
        dataloader = DataLoader(pytorch_dataset, batch_size=1, shuffle=False)
        
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= 1:  # Only test first batch
                break
                
            print(f"\nPyTorch Batch {batch_idx}:")
            for key, value in batch.items():
                if isinstance(value, torch.Tensor):
                    print(f"  {key}: {value.shape} ({value.dtype})")
                else:
                    print(f"  {key}: {value}")
        
        print("\n‚úÖ Berkeley dataset parser test completed successfully!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Main testing function"""
    success = test_berkeley_parser()
    if success:
        print("\nüéØ Ready for full Berkeley dataset training!")
    else:
        print("\n‚ö†Ô∏è Parser needs debugging before full training")

if __name__ == "__main__":
    main()


